---
title: "Tutorial 1"
output:
  md_document:
    variant: markdown_github
    toc: true
    pandoc_args: ["--extract-media", "docs/assets/images"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Per questa serie di tutorial, useremo l'__Emotions Dataset__, un classico esempio di _Supervised Classification_. 
Curato da [Hari Shankar](https://www.linkedin.com/in/this-is-hari-shankar/) e disponibile sulla piattaforma [Hugging Face](https://huggingface.co/datasets/boltuix/emotions-dataset). 
Viene rilasciato con licenza MIT. 

## Struttura e Caratteristiche
Il dataset è composto da una collezione di 131.306 frasi in lingua inglese ("Sentence"). La lunghezza media di queste frasi è di circa 14 parole. A ogni frase è associata un'emozione ("Label"), per un totale di 13 categorie diverse, inclusa l'opzione "Neutral" per identificare l'assenza di emozioni specifiche.

## La Distribuzione delle Classi
Un aspetto fondamentale di questo dataset è che la distribuzione delle emozioni è sbilanciata. Ecco alcune delle categorie più frequenti:
Happiness: presente in 31.205 frasi (circa il 24% del totale).
Sadness: presente in 17.809 frasi (circa il 14%).
Neutral: presente in 15.733 frasi (circa il 12%).

Le restanti categorie includono: Anger, Love, Fear, Disgust, Confusion, Surprise, Shame, Guilt, Sarcasm e Desire. Ognuna di queste ha un numero di occorrenze variabile, significativamente inferiore rispetto alle prime tre.

### Implicazioni 
Il buon senso ci suggerisce che questa distribuzione delle emozioni è quasi certamente il risultato del modo in cui il dataset è stato costruito, e non riflette la frequenza reale con cui le diverse emozioni vengono espresse nel linguaggio quotidiano.

Di conseguenza, dovremo tenere a mente due punti chiave:

Specificità del Dataset: Dobbiamo considerare la distribuzione delle classi come una caratteristica intrinseca di questo specifico dataset, non come una rappresentazione universale della probabilità delle emozioni.

Gestione dello Sbilanciamento: Sarà cruciale gestire attivamente questo sbilanciamento delle classi. Il nostro obiettivo non è "correggere" una distribuzione reale sconosciuta, ma piuttosto assicurarci che i nostri modelli abbiano abbastanza esempi per imparare efficacemente a riconoscere anche le categorie meno rappresentate, garantendo una buona capacità di generalizzazione in fase di inferenza. 

# EDA

Analisi esplorativa attraverso LDA. 



#################################################################################################################################

## Train/Valid

Setup



#################################################################################################################################

## Embeddings

Gli embeddings sono rappresentazioni vettoriali dense di entità discrete, come parole, token, frasi, o persino immagini e audio. 
Il loro scopo è catturare il significato semantico e le relazioni contestuali di queste entità in uno spazio vettoriale a bassa dimensionalità.
Un embedding per una singola parola (o token) potrebbe essere un vettore di, ad esempio, 768, 1024, 2048 o 4096 dimensioni.

Nel punto di ingresso di un LLM, c'è una "matrice di embedding" che mappa ogni token del vocabolario in un vettore di embedding. 
Il numero di parametri in questa matrice è dato da: Dimensione del Vocabolario×Dimensionalita dell’Embedding
Ad esempio, se un LLM ha un vocabolario di 50.000 token e la dimensionalità dell'embedding è 768, la matrice di embedding avrà 50.000×768=38.400.000 parametri.

Non dobbiamo confondere gli embeddings con i parametri di un LLM.
I parametri di un LLM (milioni, miliardi o trilioni) sono i pesi e i bias di tutte le connessioni neuronali all'interno della rete neurale. 
Questi includono i parametri degli strati di embedding stessi, ma anche e soprattutto i parametri delle reti Transformer (strati di attenzione, feed-forward networks, normalization layers, etc.) che processano questi embeddings.

Prova ad immaginare un LLM come se fosse un cuoco esperto.
I Parametri sono tutte le ricette che il cuoco ha imparato, le sue tecniche di taglio, le sue conoscenze sugli ingredienti, come bilanciare i sapori, ecc. 
Sono l'intera competenza del cuoco. Senza queste conoscenze (parametri), il cuoco non può cucinare nulla.
Gli Embeddings sono gli ingredienti che il cuoco ha pre-processato e organizzato in modo significativo.
Un "pomodoro" non è solo un "pomodoro" (una stringa di testo), ma è un "pomodoro" tagliato in cubetti, con una certa acidità, una certa consistenza, pronto per essere usato in una specifica ricetta.
Gli embeddings trasformano il "pomodoro" da una semplice etichetta a una rappresentazione che il cuoco (i parametri) può capire e utilizzare immediatamente in una moltitudine di modi (come parte di una salsa, in un'insalata, ecc.).

Relativamente ad embedding atti alla rappresentazione del testo, potremmo considerare:
- Embeddings statici (non contestuali): Word2Vec, GloVe, FastText. 
- Embeddings contestuali (basati su Transformer): BERT, RoBERTa, Sentence-BERT, modelli basati su GPT (es. quelli di OpenAI).

Gli embeddings contestuali sarebbero come il cuoco che capisce che lo stesso "pomodoro" avrà un ruolo e un sapore leggermente diverso se usato in una "salsa per pasta" rispetto a una "zuppa fredda", adattando la sua preparazione di conseguenza. 


###############################################################################################################

### Embeddings Utility

Esempio da fare per capire l’utilità degli Embeddings

La Selezione del Personale: Dal Metodo Tradizionale agli Embeddings
Immaginiamo un'azienda che cerca di assumere un nuovo "Sviluppatore Software Senior".
Metodo Tradizionale: Classi Predefinite e Scoring Manuale
Nel metodo tradizionale, un team HR dovrebbe seguire un processo simile a questo:
Definizione delle Classi/Ruoli: Prima di tutto, l'HR deve definire in modo esplicito la professione desiderata, ad esempio "Sviluppatore Software Senior". Questo implica creare un elenco di requisiti specifici:
Anni di esperienza (es. 5+ anni)
Linguaggi di programmazione richiesti (es. Python, Java, JavaScript)
Framework specifici (es. Django, Spring Boot, React)
Competenze trasversali (es. problem-solving, lavoro di squadra)
Livello di istruzione (es. Laurea in Informatica)
Scoring Manuale o Basato su Regole:
Ogni CV ricevuto viene letto e confrontato con questi requisiti.
Viene assegnato un punteggio ad ogni CV in base a quante delle competenze richieste sono presenti e al loro livello. Questo spesso implica la ricerca di parole chiave specifiche nel CV (es. "Python", "Django", "Senior").
Possono esserci regole complesse: "se ha esperienza con Python e Django, aggiungi X punti; se ha solo Python, aggiungi Y punti".
Rigidità: Se un candidato ha esperienza con un linguaggio molto simile ma non esplicitamente elencato (es. Kotlin invece di Java), potrebbe essere penalizzato o addirittura ignorato, a meno che l'HR non modifichi manualmente le regole o le classi.
Costo e Tempo: Questo processo è estremamente dispendioso in termini di tempo e fatica, soprattutto con centinaia o migliaia di CV. È anche soggetto a bias umani legati all'interpretazione delle parole chiave.
Dipendenza dalle Etichette: Il sistema dipende interamente dalla perfetta corrispondenza tra le etichette (parole chiave, classi) definite dall'HR e quelle presenti nel CV.
Problemi principali: Questo approccio è rigido e non coglie le sfumature del linguaggio e delle competenze. Se un candidato descrive le sue competenze in modo leggermente diverso, o ha un'esperienza rilevante ma non esattamente con le parole chiave definite, rischia di essere scartato. Non "ragiona" sul significato, ma solo sulla presenza di etichette predefinite.
Metodo Moderno: La Flessibilità e Utilità degli Embeddings
Con gli embeddings, l'approccio cambia radicalmente, passando dalla definizione rigida di classi alla comprensione delle distanze semantiche nello spazio vettoriale.
Creazione degli Embeddings di CV:
Ogni CV inviato viene trasformato in un vettore di embedding. Questo vettore cattura il significato complessivo delle esperienze, delle competenze e del percorso professionale del candidato. Parole come "Python", "programmazione orientata agli oggetti" e "sviluppo backend" si troveranno vicine nello spazio vettoriale ad altre parole e concetti semanticamente simili.
Questo processo è automatico: un LLM o un modello di embedding viene utilizzato per generare il vettore per ogni CV.
Creazione dell'Embeddings della "Professione Ideale" (Professionista Sintetico):
Invece di creare un elenco rigido di requisiti, l'HR crea una descrizione testuale del professionista ideale che sta cercando. Questa descrizione può essere un testo libero e dettagliato, ad esempio: "Cerchiamo uno sviluppatore software senior con almeno 5 anni di esperienza nel backend, profonda conoscenza di Python e dei framework web come Django o Flask. È fondamentale che abbia anche esperienza con database relazionali e non relazionali, e familiarità con i principi di DevOps e cloud computing. Desideriamo una persona proattiva, capace di lavorare in team e con forti capacità di problem-solving."
Anche questa descrizione testuale viene trasformata in un vettore di embedding. Questo vettore rappresenta il "professionista sintetico" o la "posizione ideale".
Calcolo della Distanza Semantica:
A questo punto, si calcola la distanza (o similarità) tra il vettore del "professionista sintetico" e i vettori di tutti i CV dei candidati.
I candidati i cui CV hanno vettori di embedding "più vicini" al vettore del professionista sintetico sono quelli semanticamente più rilevanti per la posizione.
Selezione e Flessibilità:
Vengono presentati all'HR i candidati che hanno i vettori di embedding più vicini alla descrizione ideale.
Flessibilità e Nuance: Un candidato che ha lavorato con un framework simile a Django, o che ha imparato le basi di DevOps in un contesto leggermente diverso, non sarà penalizzato perché il suo embedding catturerà la similarità semantica anche se le parole chiave non corrispondono perfettamente.
Se l'HR decide che i requisiti sono leggermente cambiati, basta modificare la descrizione del professionista sintetico e ricalcolare le distanze. Non è necessario riscrivere complesse regole o modificare classi.
Efficienza: Il processo di matching è rapidissimo una volta generati gli embeddings.
Vantaggi chiave:
Comprensione Semantica: Gli embeddings capiscono il significato dietro le parole, non solo le parole stesse. "Sviluppo backend" e "creazione di API" saranno vicini anche se usano termini diversi.
Flessibilità: Adattarsi a nuove esigenze è semplice come modificare una descrizione testuale.
Riduzione del Bias: Anche se i modelli possono ereditare bias dai dati di addestramento, il processo di matching basato sulla similarità riduce il bias umano diretto legato all'interpretazione manuale o alla rigidità delle regole.
Scalabilità: Permette di processare un numero enorme di CV in modo rapido ed efficiente.
Scoperta di Talenti Nascosti: Candidati che non usano le parole chiave "perfette" ma hanno competenze e esperienze altamente rilevanti vengono comunque identificati.

In conclusione, mentre il metodo tradizionale si basa su una mappatura rigida e discreta (se A allora B, se manca C allora meno punti), l'uso degli embeddings consente un approccio continuo e sfumato, dove la rilevanza è definita dalla vicinanza nello spazio del significato, rendendo il processo di selezione molto più intelligente, efficiente e adattabile.


######################################################################

### Text-to-Text Regression

L'utilità e la flessibilità degli Embeddings è una delle tracce più seguite nella ricerca sull'AI Generativa.
Poco tempo fa Google ha rilasciato questo paper: Performance Prediction for Large Systems via Text-to-Text Regression / https://arxiv.org/pdf/2506.21718

L'innovazione chiave proposta nell'articolo risiede nell'utilizzo di Large Language Models (LLM) e, implicitamente, degli embeddings, 
per trasformare il problema di predizione delle performance di sistemi software complessi in un compito di regressione text-to-text.


1. Rappresentazione del Sistema e della Richiesta
Nell'esempio HR, abbiamo trasformato un CV e una "professione ideale" in embeddings. Qui, il concetto è simile, ma applicato alla descrizione di un sistema e alla predizione della sua performance:

Descrizione del Sistema (Input): Invece di avere valori numerici discreti per ogni parametro di configurazione (es. "CPU = 8 core", "RAM = 16 GB", "Tipo di Database = PostgreSQL"), l'articolo propone di descrivere il sistema come testo libero. Per esempio: "Questo è un cluster Kubernetes con 5 nodi, ognuno con 8 CPU virtuali e 16GB di RAM, che esegue un'applicazione di microservizi basata su Spring Boot e un database PostgreSQL. Utilizza un bilanciamento del carico NGINX."

Perché gli embeddings sono utili qui? Un LLM trasforma questa descrizione testuale complessa in un vettore di embedding che cattura non solo la presenza di singoli componenti (CPU, RAM, DB), ma anche le loro relazioni contestuali e l'architettura complessiva. Ad esempio, "Kubernetes" e "microservizi" sono concetti semanticamente legati che gli embeddings possono riconoscere e posizionare vicini nello spazio vettoriale.

Descrizione della Performance Richiesta (Output/Predizione): L'obiettivo non è solo prevedere un numero (es. "latenza = 200 ms"), ma generare una descrizione testuale della performance predetta. Ad esempio: "La latenza attesa per questa configurazione è di circa 200 millisecondi con un throughput di 1000 transazioni al secondo e un utilizzo della CPU del 70%."

Questo approccio "text-to-text" sfrutta la capacità generativa degli LLM. Il modello, avendo "compreso" la configurazione tramite i suoi embeddings interni, può generare una descrizione complessa e multisfaccettata della performance.

2. Addestramento e Flessibilità del Modello
Il modello viene addestrato su coppie di dati: [Descrizione testuale del sistema] -> [Descrizione testuale della sua performance misurata].

Apprendimento delle Relazioni Complesse: Il LLM, attraverso i suoi meccanismi di attenzione e la sua enorme quantità di parametri, impara a mappare gli embeddings della descrizione del sistema agli embeddings della descrizione della performance. Questo gli permette di cogliere interazioni e dipendenze non lineari che sarebbero estremamente difficili da modellare con metodi tradizionali.

Flessibilità alla Descrizione: Proprio come nel caso HR, dove potevi descrivere il professionista ideale in modo flessibile, qui puoi descrivere la configurazione del sistema usando un linguaggio naturale. Non devi aderire a schemi di input rigidi o a liste di parametri predefinite. Se un nuovo componente o una nuova architettura emerge, puoi semplicemente descriverla.

Generalizzazione: Il modello può generalizzare a configurazioni leggermente diverse o a nuove combinazioni di componenti che non ha visto esplicitamente durante l'addestramento, basandosi sulla similarità semantica dei loro embeddings con le configurazioni già note.

3. Utilità degli Embeddings nella Predizione delle Performance
Gli embeddings agiscono come il ponte fondamentale tra il linguaggio naturale (le descrizioni del sistema e della performance) e la capacità di calcolo del modello.

Cattura di Semantica e Contesto: Permettono al modello di "capire" che "8 core" e "otto processori" significano la stessa cosa, o che "ridondanza del database" è un concetto legato alla "disponibilità del sistema", anche se i termini esatti non sono stati visti in ogni combinazione.

Gestione della Dimensionalità: Invece di lavorare con migliaia di variabili discrete e categoriche (ognuna delle quali sarebbe un "parametro" nel senso tradizionale), gli embeddings le comprimono in un vettore denso e a bassa dimensionalità, semplificando il compito di apprendimento per l'LLM.

Spazio Continuo: La predizione avviene in uno spazio continuo di significato, non in classi discrete. Questo permette di generare previsioni più sfumate e accurate, come "circa 200 ms" invece di una semplice classificazione "lento" o "veloce".

Connessione all'Esempio HR e Oltre
L'esempio HR ha mostrato come gli embeddings trasformino CV e job description in "punti" in uno spazio semantico, permettendo di trovare candidati "vicini" alla posizione ideale. L'articolo "Performance Prediction for Large Systems" estende questo concetto in modo elegante:

Input: La "descrizione testuale del sistema" è analoga al CV.

Output/Target: La "descrizione testuale della performance" è analoga alla "professione ideale" (anche se qui è il risultato della predizione).

Modello (LLM): È l'equivalente del "cuoco esperto" dell'analogia precedente, che impara le complesse relazioni tra le descrizioni delle configurazioni e le loro performance.

Flessibilità: Entrambi gli scenari beneficiano enormemente della capacità degli embeddings di gestire input complessi e variegati in linguaggio naturale, senza la necessità di pre-definire schemi rigidi o regole esaustive.

Questo articolo dimostra come la potenza degli LLM e degli embeddings stia trasformando non solo la comprensione del linguaggio, ma anche campi apparentemente distanti come l'ingegneria dei sistemi, permettendo approcci più adattivi, intuitivi e scalabili alla predizione e all'ottimizzazione.

