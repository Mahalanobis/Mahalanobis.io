<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://mahalanobis.github.io/Mahalanobis.io/T01/">
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>T01 - Fine-Tuning a small LLM (4Stats)</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/fontawesome.min.css" rel="stylesheet">
        <link href="../css/brands.min.css" rel="stylesheet">
        <link href="../css/solid.min.css" rel="stylesheet">
        <link href="../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/purebasic.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">Fine-Tuning a small LLM (4Stats)</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href=".." class="nav-link">Intro</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">DoE</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../T01_02/" class="dropdown-item">EDA</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="2"><a href="#struttura-e-caratteristiche" class="nav-link">Struttura e Caratteristiche</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#la-distribuzione-delle-classi" class="nav-link">La Distribuzione delle Classi</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#eda" class="nav-link">EDA</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#trainvalid" class="nav-link">Train/Valid</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#embeddings" class="nav-link">Embeddings</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<ul>
<li><a href="#struttura-e-caratteristiche">Struttura e Caratteristiche</a></li>
<li><a href="#la-distribuzione-delle-classi">La Distribuzione delle Classi</a><ul>
<li><a href="#implicazioni">Implicazioni</a></li>
</ul>
</li>
<li><a href="#eda">EDA</a><ul>
<li><a href="#trainvalid">Train/Valid</a></li>
<li><a href="#embeddings">Embeddings</a><ul>
<li><a href="#embeddings-utility">Embeddings Utility</a></li>
<li><a href="#text-to-text-regression">Text-to-Text Regression</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Per questa serie di tutorial, useremo l’<strong>Emotions Dataset</strong>, un
classico esempio di <em>Supervised Classification</em>. Curato da <a href="https://www.linkedin.com/in/this-is-hari-shankar/">Hari
Shankar</a> e
disponibile sulla piattaforma <a href="https://huggingface.co/datasets/boltuix/emotions-dataset">Hugging
Face</a>. Viene
rilasciato con licenza MIT.</p>
<h2 id="struttura-e-caratteristiche">Struttura e Caratteristiche</h2>
<p>Il dataset è composto da una collezione di 131.306 frasi in lingua
inglese (“Sentence”). La lunghezza media di queste frasi è di circa 14
parole. A ogni frase è associata un’emozione (“Label”), per un totale di
13 categorie diverse, inclusa l’opzione “Neutral” per identificare
l’assenza di emozioni specifiche.</p>
<h2 id="la-distribuzione-delle-classi">La Distribuzione delle Classi</h2>
<p>Un aspetto fondamentale di questo dataset è che la distribuzione delle
emozioni è sbilanciata. Ecco alcune delle categorie più frequenti:
Happiness: presente in 31.205 frasi (circa il 24% del totale). Sadness:
presente in 17.809 frasi (circa il 14%). Neutral: presente in 15.733
frasi (circa il 12%).</p>
<p>Le restanti categorie includono: Anger, Love, Fear, Disgust, Confusion,
Surprise, Shame, Guilt, Sarcasm e Desire. Ognuna di queste ha un numero
di occorrenze variabile, significativamente inferiore rispetto alle
prime tre.</p>
<h3 id="implicazioni">Implicazioni</h3>
<p>Il buon senso ci suggerisce che questa distribuzione delle emozioni è
quasi certamente il risultato del modo in cui il dataset è stato
costruito, e non riflette la frequenza reale con cui le diverse emozioni
vengono espresse nel linguaggio quotidiano.</p>
<p>Di conseguenza, dovremo tenere a mente due punti chiave:</p>
<p>Specificità del Dataset: Dobbiamo considerare la distribuzione delle
classi come una caratteristica intrinseca di questo specifico dataset,
non come una rappresentazione universale della probabilità delle
emozioni.</p>
<p>Gestione dello Sbilanciamento: Sarà cruciale gestire attivamente questo
sbilanciamento delle classi. Il nostro obiettivo non è “correggere” una
distribuzione reale sconosciuta, ma piuttosto assicurarci che i nostri
modelli abbiano abbastanza esempi per imparare efficacemente a
riconoscere anche le categorie meno rappresentate, garantendo una buona
capacità di generalizzazione in fase di inferenza.</p>
<h1 id="eda">EDA</h1>
<p>Analisi esplorativa attraverso LDA.</p>
<h6 id="_1">###########################################################################################################################</h6>
<h2 id="trainvalid">Train/Valid</h2>
<p>Setup</p>
<h6 id="_2">###########################################################################################################################</h6>
<h2 id="embeddings">Embeddings</h2>
<p>Gli embeddings sono rappresentazioni vettoriali dense di entità
discrete, come parole, token, frasi, o persino immagini e audio. Il loro
scopo è catturare il significato semantico e le relazioni contestuali di
queste entità in uno spazio vettoriale a bassa dimensionalità. Un
embedding per una singola parola (o token) potrebbe essere un vettore
di, ad esempio, 768, 1024, 2048 o 4096 dimensioni.</p>
<p>Nel punto di ingresso di un LLM, c’è una “matrice di embedding” che
mappa ogni token del vocabolario in un vettore di embedding. Il numero
di parametri in questa matrice è dato da: Dimensione del
Vocabolario×Dimensionalita dell’Embedding Ad esempio, se un LLM ha un
vocabolario di 50.000 token e la dimensionalità dell’embedding è 768, la
matrice di embedding avrà 50.000×768=38.400.000 parametri.</p>
<p>Non dobbiamo confondere gli embeddings con i parametri di un LLM. I
parametri di un LLM (milioni, miliardi o trilioni) sono i pesi e i bias
di tutte le connessioni neuronali all’interno della rete neurale. Questi
includono i parametri degli strati di embedding stessi, ma anche e
soprattutto i parametri delle reti Transformer (strati di attenzione,
feed-forward networks, normalization layers, etc.) che processano questi
embeddings.</p>
<p>Prova ad immaginare un LLM come se fosse un cuoco esperto. I Parametri
sono tutte le ricette che il cuoco ha imparato, le sue tecniche di
taglio, le sue conoscenze sugli ingredienti, come bilanciare i sapori,
ecc. Sono l’intera competenza del cuoco. Senza queste conoscenze
(parametri), il cuoco non può cucinare nulla. Gli Embeddings sono gli
ingredienti che il cuoco ha pre-processato e organizzato in modo
significativo. Un “pomodoro” non è solo un “pomodoro” (una stringa di
testo), ma è un “pomodoro” tagliato in cubetti, con una certa acidità,
una certa consistenza, pronto per essere usato in una specifica ricetta.
Gli embeddings trasformano il “pomodoro” da una semplice etichetta a una
rappresentazione che il cuoco (i parametri) può capire e utilizzare
immediatamente in una moltitudine di modi (come parte di una salsa, in
un’insalata, ecc.).</p>
<p>Relativamente ad embedding atti alla rappresentazione del testo,
potremmo considerare: - Embeddings statici (non contestuali): Word2Vec,
GloVe, FastText. - Embeddings contestuali (basati su Transformer): BERT,
RoBERTa, Sentence-BERT, modelli basati su GPT (es. quelli di OpenAI).</p>
<p>Gli embeddings contestuali sarebbero come il cuoco che capisce che lo
stesso “pomodoro” avrà un ruolo e un sapore leggermente diverso se usato
in una “salsa per pasta” rispetto a una “zuppa fredda”, adattando la sua
preparazione di conseguenza.</p>
<h6 id="_3">#########################################################################################################</h6>
<h3 id="embeddings-utility">Embeddings Utility</h3>
<p>Esempio da fare per capire l’utilità degli Embeddings</p>
<p>La Selezione del Personale: Dal Metodo Tradizionale agli Embeddings
Immaginiamo un’azienda che cerca di assumere un nuovo “Sviluppatore
Software Senior”. Metodo Tradizionale: Classi Predefinite e Scoring
Manuale Nel metodo tradizionale, un team HR dovrebbe seguire un processo
simile a questo: Definizione delle Classi/Ruoli: Prima di tutto, l’HR
deve definire in modo esplicito la professione desiderata, ad esempio
“Sviluppatore Software Senior”. Questo implica creare un elenco di
requisiti specifici: Anni di esperienza (es. 5+ anni) Linguaggi di
programmazione richiesti (es. Python, Java, JavaScript) Framework
specifici (es. Django, Spring Boot, React) Competenze trasversali (es.
problem-solving, lavoro di squadra) Livello di istruzione (es. Laurea in
Informatica) Scoring Manuale o Basato su Regole: Ogni CV ricevuto viene
letto e confrontato con questi requisiti. Viene assegnato un punteggio
ad ogni CV in base a quante delle competenze richieste sono presenti e
al loro livello. Questo spesso implica la ricerca di parole chiave
specifiche nel CV (es. “Python”, “Django”, “Senior”). Possono esserci
regole complesse: “se ha esperienza con Python e Django, aggiungi X
punti; se ha solo Python, aggiungi Y punti”. Rigidità: Se un candidato
ha esperienza con un linguaggio molto simile ma non esplicitamente
elencato (es. Kotlin invece di Java), potrebbe essere penalizzato o
addirittura ignorato, a meno che l’HR non modifichi manualmente le
regole o le classi. Costo e Tempo: Questo processo è estremamente
dispendioso in termini di tempo e fatica, soprattutto con centinaia o
migliaia di CV. È anche soggetto a bias umani legati all’interpretazione
delle parole chiave. Dipendenza dalle Etichette: Il sistema dipende
interamente dalla perfetta corrispondenza tra le etichette (parole
chiave, classi) definite dall’HR e quelle presenti nel CV. Problemi
principali: Questo approccio è rigido e non coglie le sfumature del
linguaggio e delle competenze. Se un candidato descrive le sue
competenze in modo leggermente diverso, o ha un’esperienza rilevante ma
non esattamente con le parole chiave definite, rischia di essere
scartato. Non “ragiona” sul significato, ma solo sulla presenza di
etichette predefinite. Metodo Moderno: La Flessibilità e Utilità degli
Embeddings Con gli embeddings, l’approccio cambia radicalmente, passando
dalla definizione rigida di classi alla comprensione delle distanze
semantiche nello spazio vettoriale. Creazione degli Embeddings di CV:
Ogni CV inviato viene trasformato in un vettore di embedding. Questo
vettore cattura il significato complessivo delle esperienze, delle
competenze e del percorso professionale del candidato. Parole come
“Python”, “programmazione orientata agli oggetti” e “sviluppo backend”
si troveranno vicine nello spazio vettoriale ad altre parole e concetti
semanticamente simili. Questo processo è automatico: un LLM o un modello
di embedding viene utilizzato per generare il vettore per ogni CV.
Creazione dell’Embeddings della “Professione Ideale” (Professionista
Sintetico): Invece di creare un elenco rigido di requisiti, l’HR crea
una descrizione testuale del professionista ideale che sta cercando.
Questa descrizione può essere un testo libero e dettagliato, ad esempio:
“Cerchiamo uno sviluppatore software senior con almeno 5 anni di
esperienza nel backend, profonda conoscenza di Python e dei framework
web come Django o Flask. È fondamentale che abbia anche esperienza con
database relazionali e non relazionali, e familiarità con i principi di
DevOps e cloud computing. Desideriamo una persona proattiva, capace di
lavorare in team e con forti capacità di problem-solving.” Anche questa
descrizione testuale viene trasformata in un vettore di embedding.
Questo vettore rappresenta il “professionista sintetico” o la “posizione
ideale”. Calcolo della Distanza Semantica: A questo punto, si calcola la
distanza (o similarità) tra il vettore del “professionista sintetico” e
i vettori di tutti i CV dei candidati. I candidati i cui CV hanno
vettori di embedding “più vicini” al vettore del professionista
sintetico sono quelli semanticamente più rilevanti per la posizione.
Selezione e Flessibilità: Vengono presentati all’HR i candidati che
hanno i vettori di embedding più vicini alla descrizione ideale.
Flessibilità e Nuance: Un candidato che ha lavorato con un framework
simile a Django, o che ha imparato le basi di DevOps in un contesto
leggermente diverso, non sarà penalizzato perché il suo embedding
catturerà la similarità semantica anche se le parole chiave non
corrispondono perfettamente. Se l’HR decide che i requisiti sono
leggermente cambiati, basta modificare la descrizione del professionista
sintetico e ricalcolare le distanze. Non è necessario riscrivere
complesse regole o modificare classi. Efficienza: Il processo di
matching è rapidissimo una volta generati gli embeddings. Vantaggi
chiave: Comprensione Semantica: Gli embeddings capiscono il significato
dietro le parole, non solo le parole stesse. “Sviluppo backend” e
“creazione di API” saranno vicini anche se usano termini diversi.
Flessibilità: Adattarsi a nuove esigenze è semplice come modificare una
descrizione testuale. Riduzione del Bias: Anche se i modelli possono
ereditare bias dai dati di addestramento, il processo di matching basato
sulla similarità riduce il bias umano diretto legato all’interpretazione
manuale o alla rigidità delle regole. Scalabilità: Permette di
processare un numero enorme di CV in modo rapido ed efficiente. Scoperta
di Talenti Nascosti: Candidati che non usano le parole chiave “perfette”
ma hanno competenze e esperienze altamente rilevanti vengono comunque
identificati.</p>
<p>In conclusione, mentre il metodo tradizionale si basa su una mappatura
rigida e discreta (se A allora B, se manca C allora meno punti), l’uso
degli embeddings consente un approccio continuo e sfumato, dove la
rilevanza è definita dalla vicinanza nello spazio del significato,
rendendo il processo di selezione molto più intelligente, efficiente e
adattabile.</p>
<h6 id="_4">################################################################</h6>
<h3 id="text-to-text-regression">Text-to-Text Regression</h3>
<p>L’utilità e la flessibilità degli Embeddings è una delle tracce più
seguite nella ricerca sull’AI Generativa. Poco tempo fa Google ha
rilasciato questo paper: Performance Prediction for Large Systems via
Text-to-Text Regression / <a href="https://arxiv.org/pdf/2506.21718">https://arxiv.org/pdf/2506.21718</a></p>
<p>L’innovazione chiave proposta nell’articolo risiede nell’utilizzo di
Large Language Models (LLM) e, implicitamente, degli embeddings, per
trasformare il problema di predizione delle performance di sistemi
software complessi in un compito di regressione text-to-text.</p>
<ol>
<li>Rappresentazione del Sistema e della Richiesta Nell’esempio HR,
    abbiamo trasformato un CV e una “professione ideale” in embeddings.
    Qui, il concetto è simile, ma applicato alla descrizione di un
    sistema e alla predizione della sua performance:</li>
</ol>
<p>Descrizione del Sistema (Input): Invece di avere valori numerici
discreti per ogni parametro di configurazione (es. “CPU = 8 core”, “RAM
= 16 GB”, “Tipo di Database = PostgreSQL”), l’articolo propone di
descrivere il sistema come testo libero. Per esempio: “Questo è un
cluster Kubernetes con 5 nodi, ognuno con 8 CPU virtuali e 16GB di RAM,
che esegue un’applicazione di microservizi basata su Spring Boot e un
database PostgreSQL. Utilizza un bilanciamento del carico NGINX.”</p>
<p>Perché gli embeddings sono utili qui? Un LLM trasforma questa
descrizione testuale complessa in un vettore di embedding che cattura
non solo la presenza di singoli componenti (CPU, RAM, DB), ma anche le
loro relazioni contestuali e l’architettura complessiva. Ad esempio,
“Kubernetes” e “microservizi” sono concetti semanticamente legati che
gli embeddings possono riconoscere e posizionare vicini nello spazio
vettoriale.</p>
<p>Descrizione della Performance Richiesta (Output/Predizione): L’obiettivo
non è solo prevedere un numero (es. “latenza = 200 ms”), ma generare una
descrizione testuale della performance predetta. Ad esempio: “La latenza
attesa per questa configurazione è di circa 200 millisecondi con un
throughput di 1000 transazioni al secondo e un utilizzo della CPU del
70%.”</p>
<p>Questo approccio “text-to-text” sfrutta la capacità generativa degli
LLM. Il modello, avendo “compreso” la configurazione tramite i suoi
embeddings interni, può generare una descrizione complessa e
multisfaccettata della performance.</p>
<ol>
<li>Addestramento e Flessibilità del Modello Il modello viene addestrato
    su coppie di dati: [Descrizione testuale del sistema] -&gt;
    [Descrizione testuale della sua performance misurata].</li>
</ol>
<p>Apprendimento delle Relazioni Complesse: Il LLM, attraverso i suoi
meccanismi di attenzione e la sua enorme quantità di parametri, impara a
mappare gli embeddings della descrizione del sistema agli embeddings
della descrizione della performance. Questo gli permette di cogliere
interazioni e dipendenze non lineari che sarebbero estremamente
difficili da modellare con metodi tradizionali.</p>
<p>Flessibilità alla Descrizione: Proprio come nel caso HR, dove potevi
descrivere il professionista ideale in modo flessibile, qui puoi
descrivere la configurazione del sistema usando un linguaggio naturale.
Non devi aderire a schemi di input rigidi o a liste di parametri
predefinite. Se un nuovo componente o una nuova architettura emerge,
puoi semplicemente descriverla.</p>
<p>Generalizzazione: Il modello può generalizzare a configurazioni
leggermente diverse o a nuove combinazioni di componenti che non ha
visto esplicitamente durante l’addestramento, basandosi sulla similarità
semantica dei loro embeddings con le configurazioni già note.</p>
<ol>
<li>Utilità degli Embeddings nella Predizione delle Performance Gli
    embeddings agiscono come il ponte fondamentale tra il linguaggio
    naturale (le descrizioni del sistema e della performance) e la
    capacità di calcolo del modello.</li>
</ol>
<p>Cattura di Semantica e Contesto: Permettono al modello di “capire” che
“8 core” e “otto processori” significano la stessa cosa, o che
“ridondanza del database” è un concetto legato alla “disponibilità del
sistema”, anche se i termini esatti non sono stati visti in ogni
combinazione.</p>
<p>Gestione della Dimensionalità: Invece di lavorare con migliaia di
variabili discrete e categoriche (ognuna delle quali sarebbe un
“parametro” nel senso tradizionale), gli embeddings le comprimono in un
vettore denso e a bassa dimensionalità, semplificando il compito di
apprendimento per l’LLM.</p>
<p>Spazio Continuo: La predizione avviene in uno spazio continuo di
significato, non in classi discrete. Questo permette di generare
previsioni più sfumate e accurate, come “circa 200 ms” invece di una
semplice classificazione “lento” o “veloce”.</p>
<p>Connessione all’Esempio HR e Oltre L’esempio HR ha mostrato come gli
embeddings trasformino CV e job description in “punti” in uno spazio
semantico, permettendo di trovare candidati “vicini” alla posizione
ideale. L’articolo “Performance Prediction for Large Systems” estende
questo concetto in modo elegante:</p>
<p>Input: La “descrizione testuale del sistema” è analoga al CV.</p>
<p>Output/Target: La “descrizione testuale della performance” è analoga
alla “professione ideale” (anche se qui è il risultato della
predizione).</p>
<p>Modello (LLM): È l’equivalente del “cuoco esperto” dell’analogia
precedente, che impara le complesse relazioni tra le descrizioni delle
configurazioni e le loro performance.</p>
<p>Flessibilità: Entrambi gli scenari beneficiano enormemente della
capacità degli embeddings di gestire input complessi e variegati in
linguaggio naturale, senza la necessità di pre-definire schemi rigidi o
regole esaustive.</p>
<p>Questo articolo dimostra come la potenza degli LLM e degli embeddings
stia trasformando non solo la comprensione del linguaggio, ma anche
campi apparentemente distanti come l’ingegneria dei sistemi, permettendo
approcci più adattivi, intuitivi e scalabili alla predizione e
all’ottimizzazione.</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax @3/es5/tex-mml-chtml.js"></script>
        <script src="../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
