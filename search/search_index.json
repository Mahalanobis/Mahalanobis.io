{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduzione","text":"<p>Recentemente ho avuto il piacere di tenere una conferenza didattica nell\u2019ambito del corso \u201cAnalisi dei dati (data mining)\u201d, tenuto dal prof. Bruno Scarpa, presso il Dipartimento di Scienze Statistiche a Padova.</p> <p>L\u2019intervento verteva sull\u2019evoluzione che negli ultimi anni ha avuto l\u2019Intelligenza Artificiale (AI) nell\u2019ambito del NLP, un percorso affascinante che ci ha portato dalle rappresentazioni lessicali come il Bag of Words alle architetture neurali avanzate basate sui Transformer.</p> <p>Trattandosi di una discussione ad alto livello, ho sentito il desiderio di integrare quell\u2019intervento con esempi pi\u00f9 pratici e pi\u00f9 vicini all\u2019esperienza di chi, come me, proviene da quello stesso percorso formativo. E' nata cos\u00ec questa serie di tutorial,  pensati per costruire un ponte tra l\u2019AI Generativa e l\u2019AI Tradizionale.</p> <p>L\u2019architrave di questo ponte \u00e8 costituita dai problemi di Supervised Classification. Vedremo come, nel contesto dell\u2019AI Generativa, questi problemi possano essere affrontati con il fine-tuning di un Large Language Model (LLM). Un\u2019occasione unica per mettere a confronto i due mondi.</p> <p>Un aspetto rilevante di questa comparazione sar\u00e0 la misurazione dell\u2019incertezza e dell\u2019errore. Nel mondo dell\u2019AI tradizionale siamo abituati a quantificare l\u2019errore attraverso probabilit\u00e0, intervalli di confidenza e metriche derivate dai logit. Vedremo che anche nel contesto degli LLM possiamo accedere ai logit e quindi trattare l\u2019errore con la stessa familiarit\u00e0 e precisione a cui siamo abituati.</p> <p>L\u2019aspetto pratico che guider\u00e0 lo sviluppo del codice utilizzato in questi tutorial sar\u00e0 il focus sull\u2019elaborazione su GPU, essenziale per la gestione efficiente di modelli complessi che richiedono un hardware specifico. </p> <p>L\u2019invito \u00e8 sperimentare. Al di l\u00e0 delle complessit\u00e0 degli aspetti tecnici e metodologici \u00e8 un mondo pi\u00f9 accessibile di quanto uno possa pensare.</p> <p>Ringrazio i colleghi Guglielmo Strumia e Marco Cozzolino per gli spunti e il supporto; questa serie \u00e8 il frutto del lavoro di tutti noi.  Grazie ancora.</p>"},{"location":"Intro/","title":"Intro","text":"<p>Recentemente ho avuto il piacere di tenere una conferenza didattica nell\u2019ambito del corso \u201cAnalisi dei dati (data mining)\u201d, tenuto dal prof. Bruno Scarpa, presso il Dipartimento di Scienze Statistiche a Padova.</p> <p>L\u2019intervento verteva sull\u2019evoluzione che negli ultimi anni ha avuto l\u2019Intelligenza Artificiale (AI) nell\u2019ambito del NLP, un percorso affascinante che ci ha portato dalle rappresentazioni lessicali come il Bag of Words alle architetture neurali avanzate basate sui Transformer.</p> <p>Trattandosi di una discussione ad alto livello, ho sentito il desiderio di integrare quell\u2019intervento con esempi pi\u00f9 pratici e pi\u00f9 vicini all\u2019esperienza di chi, come me, proviene da quello stesso percorso formativo. Questo ha portato alla nascita di questa serie di tutorial, pensata per costruire un ponte tra l\u2019AI Generativa e l\u2019AI Tradizionale.</p> <p>L\u2019architrave di questo ponte \u00e8 costituita dai problemi di Supervised Classification. Vedremo come, nel contesto dell\u2019AI Generativa, questi problemi possano essere affrontati con il fine-tuning di un Large Language Model (LLM). Un\u2019occasione unica per mettere a confronto i due mondi.</p> <p>Un aspetto rilevante di questa comparazione sar\u00e0 la misurazione dell\u2019incertezza e dell\u2019errore. Nel mondo dell\u2019AI tradizionale siamo abituati a quantificare l\u2019errore attraverso probabilit\u00e0, intervalli di confidenza e metriche derivate dai logit. Vedremo che anche nel contesto degli LLM possiamo accedere ai logit e quindi trattare l\u2019errore con la stessa familiarit\u00e0 e precisione a cui siamo abituati.</p> <p>L\u2019aspetto pratico che guider\u00e0 lo sviluppo del codice utilizzato in questi tutorial sar\u00e0 il focus sull\u2019elaborazione su GPU, essenziale per la gestione efficiente di modelli complessi che richiedono un hardware specifico. L\u2019invito \u00e8 sperimentare. Al di l\u00e0 delle complessit\u00e0 degli aspetti tecnici e metodologici \u00e8 un mondo pi\u00f9 accessibile di quanto uno possa pensare.</p> <p>Ringrazio i colleghi Guglielmo Strumia e Marco Cozzolino per gli spunti e il supporto; questa serie \u00e8 il frutto del lavoro di tutti noi. Grazie ancora.</p>"},{"location":"T01/","title":"T01","text":"<ul> <li>Struttura e Caratteristiche</li> <li>La Distribuzione delle Classi<ul> <li>Implicazioni</li> </ul> </li> <li>EDA<ul> <li>Train/Valid</li> <li>Embeddings<ul> <li>Embeddings Utility</li> <li>Text-to-Text Regression</li> </ul> </li> </ul> </li> </ul> <p>Per questa serie di tutorial, useremo l\u2019Emotions Dataset, un classico esempio di Supervised Classification. Curato da Hari Shankar e disponibile sulla piattaforma Hugging Face. Viene rilasciato con licenza MIT.</p>"},{"location":"T01/#struttura-e-caratteristiche","title":"Struttura e Caratteristiche","text":"<p>Il dataset \u00e8 composto da una collezione di 131.306 frasi in lingua inglese (\u201cSentence\u201d). La lunghezza media di queste frasi \u00e8 di circa 14 parole. A ogni frase \u00e8 associata un\u2019emozione (\u201cLabel\u201d), per un totale di 13 categorie diverse, inclusa l\u2019opzione \u201cNeutral\u201d per identificare l\u2019assenza di emozioni specifiche.</p>"},{"location":"T01/#la-distribuzione-delle-classi","title":"La Distribuzione delle Classi","text":"<p>Un aspetto fondamentale di questo dataset \u00e8 che la distribuzione delle emozioni \u00e8 sbilanciata. Ecco alcune delle categorie pi\u00f9 frequenti: Happiness: presente in 31.205 frasi (circa il 24% del totale). Sadness: presente in 17.809 frasi (circa il 14%). Neutral: presente in 15.733 frasi (circa il 12%).</p> <p>Le restanti categorie includono: Anger, Love, Fear, Disgust, Confusion, Surprise, Shame, Guilt, Sarcasm e Desire. Ognuna di queste ha un numero di occorrenze variabile, significativamente inferiore rispetto alle prime tre.</p>"},{"location":"T01/#implicazioni","title":"Implicazioni","text":"<p>Il buon senso ci suggerisce che questa distribuzione delle emozioni \u00e8 quasi certamente il risultato del modo in cui il dataset \u00e8 stato costruito, e non riflette la frequenza reale con cui le diverse emozioni vengono espresse nel linguaggio quotidiano.</p> <p>Di conseguenza, dovremo tenere a mente due punti chiave:</p> <p>Specificit\u00e0 del Dataset: Dobbiamo considerare la distribuzione delle classi come una caratteristica intrinseca di questo specifico dataset, non come una rappresentazione universale della probabilit\u00e0 delle emozioni.</p> <p>Gestione dello Sbilanciamento: Sar\u00e0 cruciale gestire attivamente questo sbilanciamento delle classi. Il nostro obiettivo non \u00e8 \u201ccorreggere\u201d una distribuzione reale sconosciuta, ma piuttosto assicurarci che i nostri modelli abbiano abbastanza esempi per imparare efficacemente a riconoscere anche le categorie meno rappresentate, garantendo una buona capacit\u00e0 di generalizzazione in fase di inferenza.</p>"},{"location":"T01/#eda","title":"EDA","text":"<p>Analisi esplorativa attraverso LDA.</p>"},{"location":"T01/#_1","title":"###########################################################################################################################","text":""},{"location":"T01/#trainvalid","title":"Train/Valid","text":"<p>Setup</p>"},{"location":"T01/#_2","title":"###########################################################################################################################","text":""},{"location":"T01/#embeddings","title":"Embeddings","text":"<p>Gli embeddings sono rappresentazioni vettoriali dense di entit\u00e0 discrete, come parole, token, frasi, o persino immagini e audio. Il loro scopo \u00e8 catturare il significato semantico e le relazioni contestuali di queste entit\u00e0 in uno spazio vettoriale a bassa dimensionalit\u00e0. Un embedding per una singola parola (o token) potrebbe essere un vettore di, ad esempio, 768, 1024, 2048 o 4096 dimensioni.</p> <p>Nel punto di ingresso di un LLM, c\u2019\u00e8 una \u201cmatrice di embedding\u201d che mappa ogni token del vocabolario in un vettore di embedding. Il numero di parametri in questa matrice \u00e8 dato da: Dimensione del Vocabolario\u00d7Dimensionalita dell\u2019Embedding Ad esempio, se un LLM ha un vocabolario di 50.000 token e la dimensionalit\u00e0 dell\u2019embedding \u00e8 768, la matrice di embedding avr\u00e0 50.000\u00d7768=38.400.000 parametri.</p> <p>Non dobbiamo confondere gli embeddings con i parametri di un LLM. I parametri di un LLM (milioni, miliardi o trilioni) sono i pesi e i bias di tutte le connessioni neuronali all\u2019interno della rete neurale. Questi includono i parametri degli strati di embedding stessi, ma anche e soprattutto i parametri delle reti Transformer (strati di attenzione, feed-forward networks, normalization layers, etc.) che processano questi embeddings.</p> <p>Prova ad immaginare un LLM come se fosse un cuoco esperto. I Parametri sono tutte le ricette che il cuoco ha imparato, le sue tecniche di taglio, le sue conoscenze sugli ingredienti, come bilanciare i sapori, ecc. Sono l\u2019intera competenza del cuoco. Senza queste conoscenze (parametri), il cuoco non pu\u00f2 cucinare nulla. Gli Embeddings sono gli ingredienti che il cuoco ha pre-processato e organizzato in modo significativo. Un \u201cpomodoro\u201d non \u00e8 solo un \u201cpomodoro\u201d (una stringa di testo), ma \u00e8 un \u201cpomodoro\u201d tagliato in cubetti, con una certa acidit\u00e0, una certa consistenza, pronto per essere usato in una specifica ricetta. Gli embeddings trasformano il \u201cpomodoro\u201d da una semplice etichetta a una rappresentazione che il cuoco (i parametri) pu\u00f2 capire e utilizzare immediatamente in una moltitudine di modi (come parte di una salsa, in un\u2019insalata, ecc.).</p> <p>Relativamente ad embedding atti alla rappresentazione del testo, potremmo considerare: - Embeddings statici (non contestuali): Word2Vec, GloVe, FastText. - Embeddings contestuali (basati su Transformer): BERT, RoBERTa, Sentence-BERT, modelli basati su GPT (es. quelli di OpenAI).</p> <p>Gli embeddings contestuali sarebbero come il cuoco che capisce che lo stesso \u201cpomodoro\u201d avr\u00e0 un ruolo e un sapore leggermente diverso se usato in una \u201csalsa per pasta\u201d rispetto a una \u201czuppa fredda\u201d, adattando la sua preparazione di conseguenza.</p>"},{"location":"T01/#_3","title":"#########################################################################################################","text":""},{"location":"T01/#embeddings-utility","title":"Embeddings Utility","text":"<p>Esempio da fare per capire l\u2019utilit\u00e0 degli Embeddings</p> <p>La Selezione del Personale: Dal Metodo Tradizionale agli Embeddings Immaginiamo un\u2019azienda che cerca di assumere un nuovo \u201cSviluppatore Software Senior\u201d. Metodo Tradizionale: Classi Predefinite e Scoring Manuale Nel metodo tradizionale, un team HR dovrebbe seguire un processo simile a questo: Definizione delle Classi/Ruoli: Prima di tutto, l\u2019HR deve definire in modo esplicito la professione desiderata, ad esempio \u201cSviluppatore Software Senior\u201d. Questo implica creare un elenco di requisiti specifici: Anni di esperienza (es. 5+ anni) Linguaggi di programmazione richiesti (es. Python, Java, JavaScript) Framework specifici (es. Django, Spring Boot, React) Competenze trasversali (es. problem-solving, lavoro di squadra) Livello di istruzione (es. Laurea in Informatica) Scoring Manuale o Basato su Regole: Ogni CV ricevuto viene letto e confrontato con questi requisiti. Viene assegnato un punteggio ad ogni CV in base a quante delle competenze richieste sono presenti e al loro livello. Questo spesso implica la ricerca di parole chiave specifiche nel CV (es. \u201cPython\u201d, \u201cDjango\u201d, \u201cSenior\u201d). Possono esserci regole complesse: \u201cse ha esperienza con Python e Django, aggiungi X punti; se ha solo Python, aggiungi Y punti\u201d. Rigidit\u00e0: Se un candidato ha esperienza con un linguaggio molto simile ma non esplicitamente elencato (es. Kotlin invece di Java), potrebbe essere penalizzato o addirittura ignorato, a meno che l\u2019HR non modifichi manualmente le regole o le classi. Costo e Tempo: Questo processo \u00e8 estremamente dispendioso in termini di tempo e fatica, soprattutto con centinaia o migliaia di CV. \u00c8 anche soggetto a bias umani legati all\u2019interpretazione delle parole chiave. Dipendenza dalle Etichette: Il sistema dipende interamente dalla perfetta corrispondenza tra le etichette (parole chiave, classi) definite dall\u2019HR e quelle presenti nel CV. Problemi principali: Questo approccio \u00e8 rigido e non coglie le sfumature del linguaggio e delle competenze. Se un candidato descrive le sue competenze in modo leggermente diverso, o ha un\u2019esperienza rilevante ma non esattamente con le parole chiave definite, rischia di essere scartato. Non \u201cragiona\u201d sul significato, ma solo sulla presenza di etichette predefinite. Metodo Moderno: La Flessibilit\u00e0 e Utilit\u00e0 degli Embeddings Con gli embeddings, l\u2019approccio cambia radicalmente, passando dalla definizione rigida di classi alla comprensione delle distanze semantiche nello spazio vettoriale. Creazione degli Embeddings di CV: Ogni CV inviato viene trasformato in un vettore di embedding. Questo vettore cattura il significato complessivo delle esperienze, delle competenze e del percorso professionale del candidato. Parole come \u201cPython\u201d, \u201cprogrammazione orientata agli oggetti\u201d e \u201csviluppo backend\u201d si troveranno vicine nello spazio vettoriale ad altre parole e concetti semanticamente simili. Questo processo \u00e8 automatico: un LLM o un modello di embedding viene utilizzato per generare il vettore per ogni CV. Creazione dell\u2019Embeddings della \u201cProfessione Ideale\u201d (Professionista Sintetico): Invece di creare un elenco rigido di requisiti, l\u2019HR crea una descrizione testuale del professionista ideale che sta cercando. Questa descrizione pu\u00f2 essere un testo libero e dettagliato, ad esempio: \u201cCerchiamo uno sviluppatore software senior con almeno 5 anni di esperienza nel backend, profonda conoscenza di Python e dei framework web come Django o Flask. \u00c8 fondamentale che abbia anche esperienza con database relazionali e non relazionali, e familiarit\u00e0 con i principi di DevOps e cloud computing. Desideriamo una persona proattiva, capace di lavorare in team e con forti capacit\u00e0 di problem-solving.\u201d Anche questa descrizione testuale viene trasformata in un vettore di embedding. Questo vettore rappresenta il \u201cprofessionista sintetico\u201d o la \u201cposizione ideale\u201d. Calcolo della Distanza Semantica: A questo punto, si calcola la distanza (o similarit\u00e0) tra il vettore del \u201cprofessionista sintetico\u201d e i vettori di tutti i CV dei candidati. I candidati i cui CV hanno vettori di embedding \u201cpi\u00f9 vicini\u201d al vettore del professionista sintetico sono quelli semanticamente pi\u00f9 rilevanti per la posizione. Selezione e Flessibilit\u00e0: Vengono presentati all\u2019HR i candidati che hanno i vettori di embedding pi\u00f9 vicini alla descrizione ideale. Flessibilit\u00e0 e Nuance: Un candidato che ha lavorato con un framework simile a Django, o che ha imparato le basi di DevOps in un contesto leggermente diverso, non sar\u00e0 penalizzato perch\u00e9 il suo embedding catturer\u00e0 la similarit\u00e0 semantica anche se le parole chiave non corrispondono perfettamente. Se l\u2019HR decide che i requisiti sono leggermente cambiati, basta modificare la descrizione del professionista sintetico e ricalcolare le distanze. Non \u00e8 necessario riscrivere complesse regole o modificare classi. Efficienza: Il processo di matching \u00e8 rapidissimo una volta generati gli embeddings. Vantaggi chiave: Comprensione Semantica: Gli embeddings capiscono il significato dietro le parole, non solo le parole stesse. \u201cSviluppo backend\u201d e \u201ccreazione di API\u201d saranno vicini anche se usano termini diversi. Flessibilit\u00e0: Adattarsi a nuove esigenze \u00e8 semplice come modificare una descrizione testuale. Riduzione del Bias: Anche se i modelli possono ereditare bias dai dati di addestramento, il processo di matching basato sulla similarit\u00e0 riduce il bias umano diretto legato all\u2019interpretazione manuale o alla rigidit\u00e0 delle regole. Scalabilit\u00e0: Permette di processare un numero enorme di CV in modo rapido ed efficiente. Scoperta di Talenti Nascosti: Candidati che non usano le parole chiave \u201cperfette\u201d ma hanno competenze e esperienze altamente rilevanti vengono comunque identificati.</p> <p>In conclusione, mentre il metodo tradizionale si basa su una mappatura rigida e discreta (se A allora B, se manca C allora meno punti), l\u2019uso degli embeddings consente un approccio continuo e sfumato, dove la rilevanza \u00e8 definita dalla vicinanza nello spazio del significato, rendendo il processo di selezione molto pi\u00f9 intelligente, efficiente e adattabile.</p>"},{"location":"T01/#_4","title":"################################################################","text":""},{"location":"T01/#text-to-text-regression","title":"Text-to-Text Regression","text":"<p>L\u2019utilit\u00e0 e la flessibilit\u00e0 degli Embeddings \u00e8 una delle tracce pi\u00f9 seguite nella ricerca sull\u2019AI Generativa. Poco tempo fa Google ha rilasciato questo paper: Performance Prediction for Large Systems via Text-to-Text Regression / https://arxiv.org/pdf/2506.21718</p> <p>L\u2019innovazione chiave proposta nell\u2019articolo risiede nell\u2019utilizzo di Large Language Models (LLM) e, implicitamente, degli embeddings, per trasformare il problema di predizione delle performance di sistemi software complessi in un compito di regressione text-to-text.</p> <ol> <li>Rappresentazione del Sistema e della Richiesta Nell\u2019esempio HR,     abbiamo trasformato un CV e una \u201cprofessione ideale\u201d in embeddings.     Qui, il concetto \u00e8 simile, ma applicato alla descrizione di un     sistema e alla predizione della sua performance:</li> </ol> <p>Descrizione del Sistema (Input): Invece di avere valori numerici discreti per ogni parametro di configurazione (es. \u201cCPU = 8 core\u201d, \u201cRAM = 16 GB\u201d, \u201cTipo di Database = PostgreSQL\u201d), l\u2019articolo propone di descrivere il sistema come testo libero. Per esempio: \u201cQuesto \u00e8 un cluster Kubernetes con 5 nodi, ognuno con 8 CPU virtuali e 16GB di RAM, che esegue un\u2019applicazione di microservizi basata su Spring Boot e un database PostgreSQL. Utilizza un bilanciamento del carico NGINX.\u201d</p> <p>Perch\u00e9 gli embeddings sono utili qui? Un LLM trasforma questa descrizione testuale complessa in un vettore di embedding che cattura non solo la presenza di singoli componenti (CPU, RAM, DB), ma anche le loro relazioni contestuali e l\u2019architettura complessiva. Ad esempio, \u201cKubernetes\u201d e \u201cmicroservizi\u201d sono concetti semanticamente legati che gli embeddings possono riconoscere e posizionare vicini nello spazio vettoriale.</p> <p>Descrizione della Performance Richiesta (Output/Predizione): L\u2019obiettivo non \u00e8 solo prevedere un numero (es. \u201clatenza = 200 ms\u201d), ma generare una descrizione testuale della performance predetta. Ad esempio: \u201cLa latenza attesa per questa configurazione \u00e8 di circa 200 millisecondi con un throughput di 1000 transazioni al secondo e un utilizzo della CPU del 70%.\u201d</p> <p>Questo approccio \u201ctext-to-text\u201d sfrutta la capacit\u00e0 generativa degli LLM. Il modello, avendo \u201ccompreso\u201d la configurazione tramite i suoi embeddings interni, pu\u00f2 generare una descrizione complessa e multisfaccettata della performance.</p> <ol> <li>Addestramento e Flessibilit\u00e0 del Modello Il modello viene addestrato     su coppie di dati: [Descrizione testuale del sistema] -&gt;     [Descrizione testuale della sua performance misurata].</li> </ol> <p>Apprendimento delle Relazioni Complesse: Il LLM, attraverso i suoi meccanismi di attenzione e la sua enorme quantit\u00e0 di parametri, impara a mappare gli embeddings della descrizione del sistema agli embeddings della descrizione della performance. Questo gli permette di cogliere interazioni e dipendenze non lineari che sarebbero estremamente difficili da modellare con metodi tradizionali.</p> <p>Flessibilit\u00e0 alla Descrizione: Proprio come nel caso HR, dove potevi descrivere il professionista ideale in modo flessibile, qui puoi descrivere la configurazione del sistema usando un linguaggio naturale. Non devi aderire a schemi di input rigidi o a liste di parametri predefinite. Se un nuovo componente o una nuova architettura emerge, puoi semplicemente descriverla.</p> <p>Generalizzazione: Il modello pu\u00f2 generalizzare a configurazioni leggermente diverse o a nuove combinazioni di componenti che non ha visto esplicitamente durante l\u2019addestramento, basandosi sulla similarit\u00e0 semantica dei loro embeddings con le configurazioni gi\u00e0 note.</p> <ol> <li>Utilit\u00e0 degli Embeddings nella Predizione delle Performance Gli     embeddings agiscono come il ponte fondamentale tra il linguaggio     naturale (le descrizioni del sistema e della performance) e la     capacit\u00e0 di calcolo del modello.</li> </ol> <p>Cattura di Semantica e Contesto: Permettono al modello di \u201ccapire\u201d che \u201c8 core\u201d e \u201cotto processori\u201d significano la stessa cosa, o che \u201cridondanza del database\u201d \u00e8 un concetto legato alla \u201cdisponibilit\u00e0 del sistema\u201d, anche se i termini esatti non sono stati visti in ogni combinazione.</p> <p>Gestione della Dimensionalit\u00e0: Invece di lavorare con migliaia di variabili discrete e categoriche (ognuna delle quali sarebbe un \u201cparametro\u201d nel senso tradizionale), gli embeddings le comprimono in un vettore denso e a bassa dimensionalit\u00e0, semplificando il compito di apprendimento per l\u2019LLM.</p> <p>Spazio Continuo: La predizione avviene in uno spazio continuo di significato, non in classi discrete. Questo permette di generare previsioni pi\u00f9 sfumate e accurate, come \u201ccirca 200 ms\u201d invece di una semplice classificazione \u201clento\u201d o \u201cveloce\u201d.</p> <p>Connessione all\u2019Esempio HR e Oltre L\u2019esempio HR ha mostrato come gli embeddings trasformino CV e job description in \u201cpunti\u201d in uno spazio semantico, permettendo di trovare candidati \u201cvicini\u201d alla posizione ideale. L\u2019articolo \u201cPerformance Prediction for Large Systems\u201d estende questo concetto in modo elegante:</p> <p>Input: La \u201cdescrizione testuale del sistema\u201d \u00e8 analoga al CV.</p> <p>Output/Target: La \u201cdescrizione testuale della performance\u201d \u00e8 analoga alla \u201cprofessione ideale\u201d (anche se qui \u00e8 il risultato della predizione).</p> <p>Modello (LLM): \u00c8 l\u2019equivalente del \u201ccuoco esperto\u201d dell\u2019analogia precedente, che impara le complesse relazioni tra le descrizioni delle configurazioni e le loro performance.</p> <p>Flessibilit\u00e0: Entrambi gli scenari beneficiano enormemente della capacit\u00e0 degli embeddings di gestire input complessi e variegati in linguaggio naturale, senza la necessit\u00e0 di pre-definire schemi rigidi o regole esaustive.</p> <p>Questo articolo dimostra come la potenza degli LLM e degli embeddings stia trasformando non solo la comprensione del linguaggio, ma anche campi apparentemente distanti come l\u2019ingegneria dei sistemi, permettendo approcci pi\u00f9 adattivi, intuitivi e scalabili alla predizione e all\u2019ottimizzazione.</p>"},{"location":"T01_01/","title":"Dataset","text":"<ul> <li>Data<ul> <li>Struttura e Caratteristiche</li> <li>La Distribuzione delle Classi<ul> <li>Implicazioni</li> </ul> </li> </ul> </li> </ul>"},{"location":"T01_01/#data","title":"Data","text":"<p>Per questa serie di tutorial, useremo l\u2019Emotions Dataset, un classico esempio di Supervised Classification. Curato da Hari Shankar e disponibile sulla piattaforma Hugging Face. Viene rilasciato con licenza MIT.</p>"},{"location":"T01_01/#struttura-e-caratteristiche","title":"Struttura e Caratteristiche","text":"<p>Il dataset \u00e8 composto da una collezione di 131.306 frasi in lingua inglese (\u201cSentence\u201d). La lunghezza media di queste frasi \u00e8 di circa 14 parole. A ogni frase \u00e8 associata un\u2019emozione (\u201cLabel\u201d), per un totale di 13 categorie diverse, inclusa l\u2019opzione \u201cNeutral\u201d per identificare l\u2019assenza di emozioni specifiche.</p>"},{"location":"T01_01/#la-distribuzione-delle-classi","title":"La Distribuzione delle Classi","text":"<p>Un aspetto fondamentale di questo dataset \u00e8 che la distribuzione delle emozioni \u00e8 sbilanciata. Ecco alcune delle categorie pi\u00f9 frequenti: Happiness: presente in 31.205 frasi (circa il 24% del totale). Sadness: presente in 17.809 frasi (circa il 14%). Neutral: presente in 15.733 frasi (circa il 12%).</p> <p>Le restanti categorie includono: Anger, Love, Fear, Disgust, Confusion, Surprise, Shame, Guilt, Sarcasm e Desire. Ognuna di queste ha un numero di occorrenze variabile, significativamente inferiore rispetto alle prime tre.</p>"},{"location":"T01_01/#implicazioni","title":"Implicazioni","text":"<p>Il buon senso ci suggerisce che questa distribuzione delle emozioni \u00e8 quasi certamente il risultato del modo in cui il dataset \u00e8 stato costruito, e non riflette la frequenza reale con cui le diverse emozioni vengono espresse nel linguaggio quotidiano.</p> <p>Di conseguenza, dovremo tenere a mente due punti chiave:</p> <p>Specificit\u00e0 del Dataset: Dobbiamo considerare la distribuzione delle classi come una caratteristica intrinseca di questo specifico dataset, non come una rappresentazione universale della probabilit\u00e0 delle emozioni.</p> <p>Gestione dello Sbilanciamento: Sar\u00e0 cruciale gestire attivamente questo sbilanciamento delle classi. Il nostro obiettivo non \u00e8 \u201ccorreggere\u201d una distribuzione reale sconosciuta, ma piuttosto assicurarci che i nostri modelli abbiano abbastanza esempi per imparare efficacemente a riconoscere anche le categorie meno rappresentate, garantendo una buona capacit\u00e0 di generalizzazione in fase di inferenza.</p>"},{"location":"T01_02/","title":"EDA","text":"<ul> <li>EDA</li> </ul>"},{"location":"T01_02/#eda","title":"EDA","text":"<p>Analisi esplorativa attraverso LDA.</p>"}]}