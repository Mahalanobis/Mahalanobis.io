{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fine-Tuning a small LLM (using a statistical approach)","text":"<p>Recentemente ho avuto il piacere di tenere una conferenza didattica nell'ambito del corso \"Analisi dei dati (data mining)\", tenuto dal prof. Bruno Scarpa, presso il Dipartimento di Scienze Statistiche a Padova. </p> <p>L'intervento verteva sull'evoluzione che negli ultimi anni ha avuto l'Intelligenza Artificiale (AI) nell'ambito del NLP, un percorso affascinante che ci ha portato dalle rappresentazioni lessicali come il Bag of Words alle architetture neurali avanzate basate sui Transformer.</p> <p>Trattandosi di una discussione ad alto livello, ho sentito il desiderio di integrare quell'intervento con esempi pi\u00f9 pratici e pi\u00f9 vicini all'esperienza di chi, come me, proviene da quello stesso percorso formativo. Questo ha portato alla nascita di questa serie di tutorial, pensata per costruire un ponte tra l'AI Generativa e l'AI Tradizionale.</p> <p>L'architrave di questo ponte \u00e8 costituita dai problemi di Supervised Classification.  Vedremo come, nel contesto dell'AI Generativa, questi problemi possano essere affrontati con il fine-tuning di un Large Language Model (LLM). Un'occasione unica per mettere a confronto i due mondi.</p> <p>Un aspetto rilevante di questa comparazione sar\u00e0 la misurazione dell'incertezza e dell'errore.  Nel mondo dell'AI tradizionale siamo abituati a quantificare l'errore attraverso probabilit\u00e0, intervalli di confidenza e metriche derivate dai logit.  Vedremo che anche nel contesto degli LLM possiamo accedere ai logit e quindi trattare l'errore con la stessa familiarit\u00e0 e precisione a cui siamo abituati. </p> <p>L'aspetto pratico che guider\u00e0 lo sviluppo del codice utilizzato in questi tutorial sar\u00e0 il focus sull'elaborazione su GPU, essenziale per la gestione efficiente di modelli complessi che richiedono un hardware specifico. L'invito \u00e8 sperimentare. Al di l\u00e0 delle complessit\u00e0 degli aspetti tecnici e metodologici \u00e8 un mondo pi\u00f9 accessibile di quanto uno possa pensare.</p> <p>Ringrazio i colleghi Guglielmo Strumia e Marco Cozzolino per gli spunti e il supporto; questa serie \u00e8 il frutto del lavoro di tutti noi. Grazie ancora.</p>"},{"location":"Intro/","title":"Introduzione","text":"<p>Recentemente ho avuto il piacere di tenere una conferenza didattica nell\u2019ambito del corso \u201cAnalisi dei dati (data mining)\u201d, tenuto dal prof. Bruno Scarpa, presso il Dipartimento di Scienze Statistiche a Padova.</p> <p>L\u2019intervento verteva sull\u2019evoluzione che negli ultimi anni ha avuto l\u2019Intelligenza Artificiale (AI) nell\u2019ambito del NLP, un percorso affascinante che ci ha portato dalle rappresentazioni lessicali come il Bag of Words alle architetture neurali avanzate basate sui Transformer.</p> <p>Trattandosi di una discussione ad alto livello, ho sentito il desiderio di integrare quell\u2019intervento con esempi pi\u00f9 pratici e pi\u00f9 vicini all\u2019esperienza di chi, come me, proviene da quello stesso percorso formativo. Questo ha portato alla nascita di questa serie di tutorial, pensata per costruire un ponte tra l\u2019AI Generativa e l\u2019AI Tradizionale.</p> <p>L\u2019architrave di questo ponte \u00e8 costituita dai problemi di Supervised Classification. Vedremo come, nel contesto dell\u2019AI Generativa, questi problemi possano essere affrontati con il fine-tuning di un Large Language Model (LLM). Un\u2019occasione unica per mettere a confronto i due mondi.</p> <p>Un aspetto rilevante di questa comparazione sar\u00e0 la misurazione dell\u2019incertezza e dell\u2019errore. Nel mondo dell\u2019AI tradizionale siamo abituati a quantificare l\u2019errore attraverso probabilit\u00e0, intervalli di confidenza e metriche derivate dai logit. Vedremo che anche nel contesto degli LLM possiamo accedere ai logit e quindi trattare l\u2019errore con la stessa familiarit\u00e0 e precisione a cui siamo abituati.</p> <p>L\u2019aspetto pratico che guider\u00e0 lo sviluppo del codice utilizzato in questi tutorial sar\u00e0 il focus sull\u2019elaborazione su GPU, essenziale per la gestione efficiente di modelli complessi che richiedono un hardware specifico. L\u2019invito \u00e8 sperimentare. Al di l\u00e0 delle complessit\u00e0 degli aspetti tecnici e metodologici \u00e8 un mondo pi\u00f9 accessibile di quanto uno possa pensare.</p> <p>Ringrazio i colleghi Guglielmo Strumia e Marco Cozzolino per gli spunti e il supporto; questa serie \u00e8 il frutto del lavoro di tutti noi. Grazie ancora.</p>"}]}